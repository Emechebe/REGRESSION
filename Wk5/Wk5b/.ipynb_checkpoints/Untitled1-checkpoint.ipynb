{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we are going to be implementing Lasso via the coordinate descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember we used L2 ridge regression to reduce our coefficients or weights of our features\n",
    "# L2 is the sum of the squared coefficients and thats used to reduce the magnitude of the weights\n",
    "# The size of this reduction is regulated by a constant called lambda\n",
    "# So the value for reduction is given by lambda multiplied by the sum of squared coefficients\n",
    "# Users often choose this lambda and this is a very critical choise\n",
    "# So thats called ridge regularisation \n",
    "# However, this regularisation never reduces any of the coefficients to zero\n",
    "# So that means using this form of regularisation, all our features will always be included in the model\n",
    "# as none of them will ever be assigned a coefficient or weight of zero\n",
    "# This is fine if the features are all relevant. However, note that including all the features \n",
    "# for a data set that has many features will be computationally expensive.\n",
    "# Sometimes not all features are relevant for prediction\n",
    "# and sometimes some features might just be redundant\n",
    "# So if we find a way to assign zero values to non-relevant or redundant features\n",
    "# that will speed up our computation as well as yield a better model, hopefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One way to do this is called L1 Lasso \n",
    "# L2 is the sum of the squared coefficients. Thats what we used for ridge regression\n",
    "# L1 is different slightly. We take the sum of the aboslute value of the weights of the features\n",
    "# and thats what we use to regularise and this is called Lasso regression\n",
    "# Again as in L1, we need the lambda to control the regularisation\n",
    "# It turns out that in this method, at some lambda value, some weights get down to zero \n",
    "# and thus do not contribute to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this notebook, you will implement your very own LASSO solver via coordinate descent. You will:\n",
    "# Write a function to normalize features\n",
    "# Implement coordinate descent for LASSO\n",
    "# Explore effects of L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Whats the coordinate descent algorithm\n",
    "# This algorithm minimizes some function; in this case we are minimizing the weights\n",
    "# One feature of the coordinate descent algorithm is that it does not minimize all w's at the same time\n",
    "# Set a condition for convergence i.e while not converged\n",
    "# Then start the minimization:\n",
    "# It minimizes w's one by one. So you will basically set a loop that will go through the features one by one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To minimize for each feature in the loop, what you need to do is:\n",
    "# to compute a term called pj. \n",
    "# To compute pj, we\n",
    "# 1) Get the difference between the actual value and the predicted value if the current feature was omitted\n",
    "# 2) Multiply by the normalized features\n",
    "# 3) Then get the sum \n",
    "# 4) Now set the weight of our current feature to this value of pj\n",
    "# 5) This pj value is actually an indication of how much important a particular feature is for prediction\n",
    "# 6) If the feature is not that important , then the true value and the prediction without the feature will be close\n",
    "# 7) or at least as close as possible\n",
    "# 8) In that scenario, the difference will be small and when you set the weight to this small pj, \n",
    "# 9) you have effectively set the weight of this feature to be very very small and thus contribute minimally\n",
    "# 10) In the reverse case, when prediction without the current feature makes a huge difference from the actual value\n",
    "# 11) then you have a huge value as pj and when you set the weight to that pj, you are effectively\n",
    "# 12) making the weight of that feature to be huge and thus play a huge role in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in house sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1479232129.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to emechebe@ohsu.edu and will expire on June 11, 2017.\n"
     ]
    }
   ],
   "source": [
    "sales = graphlab.SFrame('kc_house_data.gl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the dataset, 'floors' was defined with type string, \n",
    "# so we'll convert them to int, before using it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales['floors'] = sales['floors'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert our sframe into numpy array\n",
    "# Luckily we already have written a function called get_numpy in Wk2, so we just copy and paste that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features,output):\n",
    "    # This function takes a data set(data_sframe), a list of features (features) and what you want to predict as a string.\n",
    "    # It returns back 2 numpy array that has the measurements of your selected features (feature_matrix)\n",
    "    #  The other array is what you want to predict (output array)\n",
    "    # Using the data we have, add a constant variable for intercept and select the features you want to use\n",
    "    data_sframe['constant'] = 1 # add a constant column to an SFrame. This is for intercept\n",
    "    features = ['constant'] + features  # Prepending the new constant variable to the features also\n",
    "    features_sframe = data_sframe[features] # Getting the newly formed user selected features Sframe\n",
    "    features_matrix = features_sframe.to_numpy() # Converting the features Sframe data to a numpy array data\n",
    "    output_sarray = data_sframe[output]\n",
    "    output_array = output_sarray.to_numpy()\n",
    "    return (features_matrix,output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also we need a function that predicts values when given a feature matrix and weights of the features\n",
    "# Again we wrote that function called predict_output. So we just copy and paste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_outcome (feature_matrix, weights):\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the house dataset, features vary wildly in their relative magnitude: \n",
    "# sqft_living is very large overall compared to bedrooms, for instance. \n",
    "# As a result, weight for sqft_living would be much smaller than weight for bedrooms. \n",
    "# This is problematic because \"small\" weights are dropped first as l1_penalty goes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To give equal considerations for all features, we need to normalize features as discussed in the lectures: \n",
    "# we divide each feature by its 2-norm so that the transformed feature has norm 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how we can do this normalization easily with Numpy: let us first consider a small matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.   5.   8.]\n",
      " [  4.  12.  15.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[3.,5.,8.],[4.,12.,15.]])\n",
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy provides a shorthand for computing 2-norms of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.  13.  17.]\n"
     ]
    }
   ],
   "source": [
    "norms = np.linalg.norm(X, axis=0) # gives [norm(X[:,0]), norm(X[:,1]), norm(X[:,2])]\n",
    "print norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So the data contains 3 features (3 columns) and you want to compute the norm of the features\n",
    "# Thats why you set the axis to 0 to do the computation column wise\n",
    "# Then what np.linalg.norm does is this:\n",
    "# 1) It gets the square of all the values\n",
    "# 2) Then it does either a row by row sum or a column by column sum (We need the latter hence the axis set at 0)\n",
    "# 3) Then once it gets the sum for each column, it then takes the square root and thats ur 2-norm\n",
    "# 4) Since we have three features, we have 3 values with each norm for each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can now use the norms for each features to normalize all the values of that particular feature\n",
    "# This is done by just dividing the matrix with the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6         0.38461538  0.47058824]\n",
      " [ 0.8         0.92307692  0.88235294]]\n"
     ]
    }
   ],
   "source": [
    "print X / norms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the shorthand we just covered, write a short function called normalize_features(feature_matrix), \n",
    "# which normalizes columns of a given feature matrix. \n",
    "# The function should return a pair (normalized_features, norms), \n",
    "# where the second item contains the norms of original features. \n",
    "# As discussed in the lectures, we will use these norms to normalize the test data \n",
    "# in the same way as we normalized the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_features(feature_matrix):\n",
    "    # Use the linalg.norm function to return the magnitude of the vector\n",
    "    # This is done column wise and thus we set the axis to 0\n",
    "    norms = np.linalg.norm(feature_matrix, axis=0) \n",
    "    # Use the norms to now divide all the values by this norm\n",
    "    features = feature_matrix / norms\n",
    "    # Return the normalized data and also the norm value that we used to generate \n",
    "    return features, norms\n",
    "\n",
    "    # Note we will use the same norm value to get the normalized value for the test data and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6  0.6  0.6]\n",
      " [ 0.8  0.8  0.8]]\n"
     ]
    }
   ],
   "source": [
    "features, norms = normalize_features(np.array([[3.,6.,9.],[4.,8.,12.]]))\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.  10.  15.]\n"
     ]
    }
   ],
   "source": [
    "print norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have a function that normalizes our data set. \n",
    "# This will aloow us to use normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing Coordinate Descent with normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note when we discussed corodinate algorithm to minimize weight,\n",
    "# we calculated a term called pj for each feature \n",
    "# and set the weight of that feature to the pj calculated.\n",
    "# This is actually an unregularised form \n",
    "# where pj is just the value calculated\n",
    "# However, what is used is the regularised form where instead of \n",
    "# just setting the weight of the current feature to the pj,\n",
    "# the pj is first regularised and that reduces the value before it is then set as the weight of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So how do we regularise our pj before setting it as the weight of the feature in which it was calculated for?\n",
    "# We use the term lambda for this. So the user sets a lambda constant\n",
    "# Now we calculate pj\n",
    "# Then ask if pj falls between -lambda/2 and lambda/2. If pj falls between that,\n",
    "# then we just decide that this feature for which the pj is calculated is not relevant.\n",
    "# Once that determination is made we then just set that weight to 0\n",
    "# Now pj might not be between this range and if it is not then we have two options\n",
    "# pj can be less than -lambda/2 and if that is the case , then we regularise pj by\n",
    "# pj -(-lambda/2) which will be pj + lambda/2. We then set the weight of this feature to the regularised value\n",
    "# The other option is that pj greater than lambda/2 and if that is the case, then we regularise pj by \n",
    "# pj -(lambda/2) which will be pj - lambda/2. We then set the weight of this feature to that regularised feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok, now that we have all the concepts , lets dig into the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to be using a slightly different formula to calculate pj\n",
    "# This formula is given by pj[i] = SUM[ [feature_i]*(output - prediction + w[i]*[feature_i]) ]\n",
    "# Lets break down this formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to calculate pj feature by feature\n",
    "# Our first computation will be w[i]*[feature_i]:\n",
    "# Here we are taking the assigned weight of the current feature and multiplying it by all the observations\n",
    "# recorded for that feature row by row. So basically you isoalte the column that contains the current feature\n",
    "# and then you multiply the weight assigned to that feature to all the rows present in that column\n",
    "# Once you have those values, you then add it to the predicted values gotten from using all the features\n",
    "# I am asuming the result of this is the predicted values when you ignore the current feature \n",
    "# Ok. Now that we have the prediction without the current feature, we now ask how important is that\n",
    "# feature we just omitted to the predictive power of the model\n",
    "# To answer that we subtract that current prediction from the actual value we are trying to predict.\n",
    "# Then we multiply those differences by the rows in the column that contains the current feature\n",
    "# Now sum up all those values and that sum is the pj for that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets illustrate this computation using just two features and calculating pj for each of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets use sqft_living and # bedrooms as our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living', 'bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want to predict the price of our houses based on these 2 features.\n",
    "# So lets create the price column, which is called my_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_output = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So lets use our get_numpy function to retrun back a feature matrix as well as an output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_feature_matrix, output = get_numpy_data(sales, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our matrix contains the raw values of the features. We want to use normalized values for this\n",
    "# So lets use our normalize_features function to return back a normalized matrix\n",
    "# Also return back the normalizer too (norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_feature_matrix, norms = normalize_features(simple_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the formula for pj, we need weights. So what we do here is just to assign random values as weights to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We assign some random set of initial weights and inspect the values of pj[i]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.array([1., 4., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note we have two features but our weight has 3. This is because w[0] is the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note also in the pj computation, we need the predictions that was made using all the features\n",
    "# Now that we already have some random weights for our feature , we can then use our predict_output function\n",
    "# This is a function that takes in the weight and the matrix as input and returns predictions\n",
    "# based on the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use predict_output() to make predictions on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction =  predict_outcome (simple_feature_matrix, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02675867,  0.04339256,  0.01990703, ...,  0.02289873,\n",
       "        0.03178473,  0.02289873])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note the actual prices are in the variable called output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 221900.,  538000.,  180000., ...,  402101.,  400000.,  325000.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets remind ourselves of the formula for pj\n",
    "# pj[i] = SUM[ [feature_i]*(output - prediction + w[i]*[feature_i]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First lets calculate w[i]*[feature_i] for feature 1 in our data set\n",
    "# Lets store that value in a variable called Feature1 \n",
    "# We can extract the column that contains all the observations for feature 1 in our matrix by this function\n",
    "# simple_feature_matrix[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature1 = weights[1] * simple_feature_matrix[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets calculate prediction + w[i]*[feature_i] by adding Predictions to Feature1\n",
    "# Lets call this Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Value = prediction + Feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04087953,  0.07414731,  0.02912149, ...,  0.0351049 ,\n",
       "        0.05093166,  0.0351049 ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets calculate output - prediction + w[i]*[feature_i] by just subtracting Value from output\n",
    "# Lets call this Value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Value2 = output - Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 221899.95912047,  537999.92585269,  179999.97087851, ...,\n",
       "        402100.9648951 ,  399999.94906834,  324999.9648951 ])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets calculate feature_i * output - prediction + w[i]*[feature_i] by just multiplying Value2 with \n",
    "# all its corresponding values in the current feature ie feature_i\n",
    "# Lets call this Rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rho = simple_feature_matrix[:,1] * Value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  783.3545591 ,  4136.51387074,   414.65060734, ...,  1227.02788605,\n",
       "        1914.69262512,   991.75096482])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now finally lets get pj by getting the sum of Rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pj = sum(Rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87939462.77299127"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So the value above is the pj value , given the assigned weights, for Feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we want to repeat this for feature2\n",
    "# But instead of going through all this, lets wrap all this computation in a function\n",
    "# Lets call it pj_calculatorV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pj_calculatorV1(feature_matrix, weights):\n",
    "    # Initializing the weights\n",
    "    #weights = np.array([1., 4., 1.])\n",
    "    \n",
    "    # Getting the predictions\n",
    "    prediction =  predict_outcome (feature_matrix, weights)\n",
    "    \n",
    "    # Calculating the pj for each feature in the feature matrix\n",
    "    for i in xrange(len(weights)):\n",
    "        Feature = weights[i] * simple_feature_matrix[:,i]\n",
    "        Value = prediction + Feature\n",
    "        Value2 = output - Value\n",
    "        Rho = simple_feature_matrix[:,i] * Value2\n",
    "        pj = sum(Rho)\n",
    "        print pj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79400298.0349\n",
      "87939462.773\n",
      "80966696.676\n"
     ]
    }
   ],
   "source": [
    "pj_calculatorV1(simple_feature_matrix, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now you see we can calculate pj using the pj_calculator\n",
    "# All we need is normalized feature matrix and initialized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok , remenber that the reason why we calculate pj is to determine if\n",
    "#1) We should set the weight to zero\n",
    "#2) If we should add to the pj and then set it to the weight (ie pj very negative)\n",
    "#3) Or if we should subtract from the pj and then set it to the weight (ie pj very positive)\n",
    "#4) As we have said before, this is called regularisation of the pj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To regularise, we need the lambda value \n",
    "# If the value of pj falls between -lambda/2 and lambda/2 , then we set the weight of that particular feature\n",
    "# for which this pj was calculated to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok, for feature 1 whose pj is 87939462.773. What lambda value would lead to setting this feature to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lambda = 87939462.773 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175878925.546"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So if we chose this lambda, then the pj will be between -175878925.546/2 and 175878925.546/2\n",
    "# and thus we set feature1 to zero given this lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok, for feature 2 whose pj is 80966696.676 . What lambda value would lead to setting this feature to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lambda = 80966696.676 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161933393.352"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So if we chose this lambda, then the pj will be between -161933393.352/2 and 161933393.352/2\n",
    "# and thus we set feature1 to zero given this lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz: What range of values of l1_penalty would not set w[1] zero, \n",
    "# but would set w[2] to zero, if we were to take a step in that coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = (1.479879e+08, 1.758789e+08)\n"
     ]
    }
   ],
   "source": [
    "diff = abs((87939462.773*2) - (80966696.676*2))\n",
    "print('Lambda = (%e, %e)' %((80966696.676-diff/2+1)*2, (80966696.676+diff/2-1)*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So we can say that pj[i] quantifies the significance of the i-th feature: \n",
    "# the larger pj[i] is, the more likely it is for the i-th feature to be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets implement the coordinate descent algorithm to get the minimized weights of features\n",
    "# Note this co oridnate descent algorithm is just an extension of our pj_calculator\n",
    "# The difference is that we dont return the pj, but we use it to set the weight\n",
    "# To do that we need the Lambda penalty and test if the pj is between -lamda/2 and lamda/2. If so, set to 0\n",
    "# Else set weight to pj - lamda/2 if pj is greater than lambda/2\n",
    "# or set weight to pj + lamda/2 if pj is leaa than -lamda/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets illustrate this by just writing a cordinate descent function that will\n",
    "# minimize for just one feature of a feature matrix.\n",
    "# So in addition to the inputs that we used for the pj_calculator (simple_feature_matrix, weights), we\n",
    "# need the column number signifying the feature that we want i\n",
    "# We also need the lambda which we call the l1_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lasso_coordinate_descent_stepV1(i, feature_matrix, output, weights, l1_penalty):\n",
    "    # Getting the predictions\n",
    "    prediction =  predict_outcome (feature_matrix, weights)\n",
    "    \n",
    "    # Calculating the pj for each feature in the feature matrix\n",
    "    #Feature = weights[i] * feature_matrix[:,i]\n",
    "    #Value = prediction + Feature\n",
    "    #Value2 = output - Value\n",
    "    #Rho = feature_matrix[:,i] * Value2\n",
    "    #pj = sum(Rho)\n",
    "    # For some reason my computation of pj using the above method was no longer working.\n",
    "    # This below gave me the result that I wanted from the test run. I have to look into it and \n",
    "    # and ask whats wrong with the way I was doing it initially. Thats a bit worrisome\n",
    "    \n",
    "    # Calculating pj\n",
    "    pj = np.sum(feature_matrix[:,i]*(output - prediction + weights[i]*feature_matrix[:,i]))\n",
    "    \n",
    "    \n",
    "    # Regularising pj\n",
    "    if i == 0 : # This is intercept and so do not regularise\n",
    "            New_weight = pj\n",
    "    \n",
    "    \n",
    "    elif pj < -l1_penalty/2.: #ie pj is a very big negative number relative to lambda\n",
    "            New_weight = pj + (l1_penalty/2)\n",
    "    \n",
    "    \n",
    "    elif pj > l1_penalty/2.: #ie pj is a very big positive number relative to lambda\n",
    "            New_weight = pj - (l1_penalty/2)\n",
    "    \n",
    "    \n",
    "    else:                   #ie pj lies between -lambda/2 and lambda/2\n",
    "            New_weight = 0\n",
    "    return New_weight\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.425558846691\n"
     ]
    }
   ],
   "source": [
    "print lasso_coordinate_descent_stepV1(1, np.array([[3./math.sqrt(13),1./math.sqrt(10)],[2./math.sqrt(13),3./math.sqrt(10)]]), \n",
    "                                   np.array([1., 1.]), np.array([1., 4.]), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have the lasso_coordinate_descent_stepV1 that minimizes the weight for just one feature\n",
    "# Now instead of just doing it for one feature, we want to do it for all features in the dataset\n",
    "# We can modify this function , so that it loops through the features and performs the optimization for each feature\n",
    "# This is exactly what I did here I just added a loop to go through each feature. I used the number of the weights to\n",
    "# set up this loop. \n",
    "# It only goes through all the features once and then returns a list of weights that is the new optimized weights\n",
    "# for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lasso_coordinate_descent_stepV2( feature_matrix, output, weights, l1_penalty):\n",
    "    \n",
    "    # Getting the predictions\n",
    "    prediction =  predict_outcome (feature_matrix, weights)\n",
    "    \n",
    "    # Calculating the pj for each feature in the feature matrix\n",
    "    #Feature = weights[i] * feature_matrix[:,i]\n",
    "    #Value = prediction + Feature\n",
    "    #Value2 = output - Value\n",
    "    #Rho = feature_matrix[:,i] * Value2\n",
    "    #pj = sum(Rho)\n",
    "    # For some reason my computation of pj using the above method was no longer working.\n",
    "    # This below gave me the result that I wanted from the test run. I have to look into it and \n",
    "    # and ask whats wrong with the way I was doing it initially. Thats a bit worrisome\n",
    "    \n",
    "    \n",
    "    New_weights = []  # Initiating an empty New_weights array to append all the weights of the features\n",
    "    \n",
    "    for i in xrange(len(weights)):  # Setting a loop to go  through all the features\n",
    "         # Calculating pj of the ith feature\n",
    "            pj = np.sum(feature_matrix[:,i]*(output - prediction + weights[i]*feature_matrix[:,i]))\n",
    "    \n",
    "    \n",
    "         # Regularising pj of the ith feature\n",
    "            if i == 0 : # This is intercept and so do not regularise\n",
    "                New_weight = pj\n",
    "    \n",
    "    \n",
    "            elif pj < -l1_penalty/2.: #ie pj is a very big negative number relative to lambda\n",
    "               New_weight = pj + (l1_penalty/2)\n",
    "    \n",
    "    \n",
    "            elif pj > l1_penalty/2.: #ie pj is a very big positive number relative to lambda\n",
    "               New_weight = pj - (l1_penalty/2)\n",
    "    \n",
    "    \n",
    "            else:                   #ie pj lies between -lambda/2 and lambda/2\n",
    "               New_weight = 0\n",
    "            New_weights.append(New_weight) # Collecting all the weights by appending them in the empty list\n",
    "    return New_weights  # return new updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets test the function to make sure it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79400300.034929156, 6972774.0969910771, 1.9999656528234482]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coordinate_descent_stepV2(simple_feature_matrix,output,weights,161933393.352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So given the initial weights, we now returned a set of optimized weights.\n",
    "# This is the optimal weight but we can re-use those new set of weights that we just calculated to make a new prediction\n",
    "# So I have to reset my prediction everytime with a new weight\n",
    "# To do this, I will have to replace the old weights with the new weights we calculated\n",
    "# This will be achieved by just saying: weights = New_weights and then set a loop that makes\n",
    "# the function run again using those new weights\n",
    "# For now I will just set up a loop called iteration that tells the function how many times to run\n",
    "# and recalculate with new weights\n",
    "# Lets modify our functions to implement these new steps\n",
    "# This function is called lasso_coordinate_descent_stepV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lasso_coordinate_descent_stepV3( feature_matrix, output, weights, l1_penalty,iteration):\n",
    "    \n",
    "    for j in xrange(iteration):  # Here I am setting the loop to determine how many rounds of optimization\n",
    "        \n",
    "        # Getting the predictions of the jth iteration\n",
    "        prediction =  predict_outcome (feature_matrix, weights)\n",
    "        \n",
    "        New_weights = [] # Initiating an empty New_weights array to append all the weights of the features\n",
    "        \n",
    "        #print weights\n",
    "        \n",
    "        for i in xrange(len(weights)):  # Setting a loop to go  through all the features\n",
    "            \n",
    "            # Calculating pj for the ith feature\n",
    "            pj = np.sum(feature_matrix[:,i]*(output - prediction + weights[i]*feature_matrix[:,i]))\n",
    "    \n",
    "    \n",
    "            # Regularising pj for the ith feature\n",
    "            if i == 0 : # This is intercept and so do not regularise\n",
    "                New_weight = pj\n",
    "    \n",
    "    \n",
    "            elif pj < -l1_penalty/2.: #ie pj is a very big negative number relative to lambda\n",
    "               New_weight = pj + (l1_penalty/2)\n",
    "    \n",
    "    \n",
    "            elif pj > l1_penalty/2.: #ie pj is a very big positive number relative to lambda\n",
    "               New_weight = pj - (l1_penalty/2)\n",
    "    \n",
    "    \n",
    "            else:                   #ie pj lies between -lambda/2 and lambda/2\n",
    "               New_weight = 0\n",
    "            \n",
    "            New_weights.append(New_weight) # Collecting all the weights by appending them in the empty list\n",
    "        \n",
    "        weights = New_weights # Resetting the old weights with the new weights. As a result, during the jth +1 \n",
    "                              # iteration, the weights used are the weights calculated from the jth iteration\n",
    "    \n",
    "    return New_weights # Once all the iterations for optimization are done, return the current weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets test ride this function with iterations of 1, 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79400300.034929156, 6972774.0969910771, 1.9999656528234482]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coordinate_descent_stepV3(simple_feature_matrix,output,weights,161933393.352,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73021713.024645835, 0, 0]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coordinate_descent_stepV3(simple_feature_matrix,output,weights,161933393.352,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79400304.65805088, 0, 0]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coordinate_descent_stepV3(simple_feature_matrix,output,weights,161933393.352,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seems to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok we used an iteration loop to tell the function how many rounds of optimization we want.\n",
    "# Thats just very arbitrailiy\n",
    "# However, instead of telling the function the number of iteration arbitraily we can be more intelligent \n",
    "# about how many time we want the function to run\n",
    "# We can use the fact that we are calculating new weights from old weights everytime we go through\n",
    "# a round of optimization. We hope that as we optimnize, the difference between the previous weights and the \n",
    "# current weight will get smaller and smaller with each round. \n",
    "# We can set a tolerance value and say once the difference between an old weight and new weight\n",
    "# becomes less than that tolerance value, then stop the programme and return the weights at that point\n",
    "# Note since the coordinate descent calculates weights feature by feature....\n",
    "# Then you have to calculate all the weights for each feature, calculate the difference between the old weight\n",
    "# and the new weight. Then you want to take the one that has the largest difference and that is the one you want to use\n",
    "# to determine convergence by asking if that largest difference is smaller than the tolerance\n",
    "# Once the algorithm finds a largest difference that is smaller than the pre specified tolerance,\n",
    "# the program is terminated and the weights at that point are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now update our algorithm and the inputs are very much the same,\n",
    "# except in this case instead of iteration, it has tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lasso_coordinate_descent_stepV4(feature_matrix, output, initial_weights, l1_penalty, tolerance):\n",
    "    weights = initial_weights.copy()    \n",
    "    # converged condition variable    \n",
    "    \n",
    "    converged  = False      # We are setting a condition that allows the algorithm to run until it is violated       \n",
    "    \n",
    "    while not converged:    # As long as this condition remains through, keep running       \n",
    "        max_change = 0      # We need to calculate the difference between the old and new weight\n",
    "                            # So we set up a variable to hold that difference\n",
    "        \n",
    "        for i in range(len(weights)):  # We now cycle through all the features made possible by using the lenght of weights\n",
    "            \n",
    "            old_weights_i = weights[i] # For each feature that we are in, we set the current weight as the old weight\n",
    "                                       # Makes sense since we are going to be calculating a new weight\n",
    "            \n",
    "            # Now we calculate the new weight for the current feature that we are in\n",
    "            # We already have written a function that calculates weight for just one feature.\n",
    "            # We call that function (ie invoke lasso_coordinate_descent_stepV1)\n",
    "            weights[i]=lasso_coordinate_descent_stepV1(i, feature_matrix, output, weights, l1_penalty)\n",
    "            \n",
    "            # Now we have an old weight and a new weight calculated from the old weights\n",
    "            # We want the absolute difference between those two weights\n",
    "            change_i = np.abs(old_weights_i - weights[i])\n",
    "            \n",
    "            # Now we know we are going to do this for all the features and get differences\n",
    "            # But the difference we want to use is the biggest difference\n",
    "            # So we write an expression that will ensure we are only saving the \n",
    "            # biggest difference as max_change\n",
    "            if change_i > max_change:                \n",
    "                max_change = change_i        \n",
    "        \n",
    "        # After this round of going through all the features, calculating each weight ,getting the difference\n",
    "        # between old and new weight and finally only keeping the one with the highest difference\n",
    "        # We can now use that to test our tolerance.\n",
    "        # If that max_change is less than the specified tolerance, then we stop and return weights\n",
    "        # Else, we just go back and repeat the whole process until we encounter a max_change that is less than tolerance\n",
    "        if max_change < tolerance:              \n",
    "            converged = True     \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living', 'bedrooms']\n",
    "my_output = 'price'\n",
    "initial_weights = np.zeros(3)\n",
    "l1_penalty = 1e7\n",
    "tolerance = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First create a normalized version of the feature matrix, normalized_simple_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(simple_feature_matrix, output) = get_numpy_data(sales, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(normalized_simple_feature_matrix, simple_norms) = normalize_features(simple_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = lasso_coordinate_descent_stepV4(normalized_simple_feature_matrix, output,\n",
    "                                            initial_weights, l1_penalty, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21624998.36636292,  63157246.78545421,         0.        ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is the RSS of the learned model on the normalized dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have our weights , we can then use our weights and our matrix data to now get predictions based on the weights\n",
    "# We have a function called predict_outcome that takes a matrix and a set of weights and returns predictions based on\n",
    "# the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictions=predict_outcome (normalized_simple_feature_matrix, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 370053.87731138,  632691.61911816,  292585.19087916, ...,\n",
       "        339822.19480125,  449412.04390048,  339822.19480125])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Those are the predictions. How close are they to the actual values?\n",
    "# Note the actual values are in array called output\n",
    "# Lets take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 221900.,  538000.,  180000., ...,  402101.,  400000.,  325000.])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets get the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Residuals = Predictions - output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now square them to get rid of negative signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Residuals_squared = Residuals * Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then sum them all up to get the RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RSS = Residuals_squared.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630492481484487.8"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which features had weight zero at convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ans = Bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the above example, we only used 2 features\n",
    "# Lets add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating LASSO fit with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us split the sales dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us consider the following set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = ['bedrooms',\n",
    "                'bathrooms',\n",
    "                'sqft_living',\n",
    "                'sqft_lot',\n",
    "                'floors',\n",
    "                'waterfront', \n",
    "                'view', \n",
    "                'condition', \n",
    "                'grade',\n",
    "                'sqft_above',\n",
    "                'sqft_basement',\n",
    "                'yr_built', \n",
    "                'yr_renovated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a normalized feature matrix from the TRAINING data with these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets get the matrix first using our numpy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_matrix_train_data, output_train_data) = get_numpy_data(train_data, all_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets do the normalization of the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(normalized_feature_matrix_train_data, simple_norms_train_data) = normalize_features(feature_matrix_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, learn the weights with l1_penalty=1e7, on the training data. \n",
    "# Initialize weights to all zeros, and set the tolerance=1. \n",
    "# Call resulting weights weights1e7, you will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_penalty=1e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights to all zeros.I have 13 features , plus intercept...so 14 in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_weights = np.zeros(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tolerance=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now learning weights of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights1e7 = lasso_coordinate_descent_stepV4(normalized_feature_matrix_train_data, output_train_data,\n",
    "                                            initial_weights, l1_penalty, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What features had non-zero weight in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24429600.60933314,         0.        ,         0.        ,\n",
       "        48389174.35227978,         0.        ,         0.        ,\n",
       "         3317511.16271982,   7329961.9848964 ,         0.        ,\n",
       "               0.        ,         0.        ,         0.        ,\n",
       "               0.        ,         0.        ])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sqft_living, waterfront, view\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, learn the weights with l1_penalty=1e8, on the training data. \n",
    "# Initialize weights to all zeros, and set the tolerance=1. \n",
    "# Call resulting weights weights1e8, you will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_penalty=1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights1e8 = lasso_coordinate_descent_stepV4(normalized_feature_matrix_train_data, output_train_data,\n",
    "                                            initial_weights, l1_penalty, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What features had non-zero weight in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 71114625.75280938,         0.        ,         0.        ,\n",
       "               0.        ,         0.        ,         0.        ,\n",
       "               0.        ,         0.        ,         0.        ,\n",
       "               0.        ,         0.        ,         0.        ,\n",
       "               0.        ,         0.        ])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1e8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, learn the weights with l1_penalty=1e4, on the training data. \n",
    "# Initialize weights to all zeros, and set the tolerance=5e5. \n",
    "# Call resulting weights weights1e4, you will need them later. \n",
    "# (This case will take quite a bit longer to converge than the others above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_penalty=1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tolerance=5e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights1e4 = lasso_coordinate_descent_stepV4(normalized_feature_matrix_train_data, output_train_data,\n",
    "                                            initial_weights, l1_penalty, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77779073.91265225, -22884012.25023361,  15348487.08089996,\n",
       "        92166869.69883074,  -2139328.0824278 ,  -8818455.54409492,\n",
       "         6494209.73310655,   7065162.05053198,   4119079.21006765,\n",
       "        18436483.52618776, -14566678.54514342,  -5528348.75179426,\n",
       "       -83591746.20730537,   2784276.46012858])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What features had non-zero weight in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok now we have used our training data to learn weights.\n",
    "# We now want to test these weights on the test_data set\n",
    "# However, we have to normalize the test_data just like we did the train_data set before we do any computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting test data as numpy matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_matrix_test_data, output_test_data) = get_numpy_data(test_data, all_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the test data using the norms we generated when we normalized train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Normalized_test_data = feature_matrix_test_data/simple_norms_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets use the first weight to predict the price in the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictions=predict_outcome (Normalized_test_data, weights1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the RSS of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals = Predictions - output_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residual_squared = residuals * residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RSS = residual_squared.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227781004760225.34"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets use the second weight to predict the price in the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictions=predict_outcome (Normalized_test_data, weights1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the RSS of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals = Predictions - output_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residual_squared = residuals * residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RSS = residual_squared.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275962079909185.28"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets use the thrid weight to predict the price in the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictions=predict_outcome (Normalized_test_data, weights1e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the RSS of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals = Predictions - output_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residual_squared = residuals * residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RSS = residual_squared.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537166150034084.88"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz question: Which model performed the best?\n",
    "# That will be the model with the least RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ans is weights1e4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
